{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Overview of the ARC Challenge\n",
        "Author: Sami Halabieh<br>",
        "\n",
        "Date: Mar 14 2025<br>",
        "\n<br>",
        "The Abstraction and Reasoning Corpus (ARC) is a benchmark of visual reasoning puzzles introduced by François Chollet. Each ARC task is a small grid-based puzzle with a few given example input-output pairs and a test input for which the solution must be inferred. There are 400 training tasks and a similar number of evaluation tasks, each with only 3–4 demonstration pairs on average​.\n",
        "\n",
        "The tasks are extremely diverse, some require pixel-level operations (e.g. changing colors in specific locations), others involve object-centric transformations (moving or rotating shapes), and others test higher level abstractions like symmetry or counting.\n",
        "\n",
        " Michael Hodel noted that one can categorize tasks by the level of transformation: grid-level (global pattern), object-level (manipulating entire objects), or cell-level (individual pixel rules). The combination of high diversity and very few examples per task makes ARC especially challenging.\n",
        "\n",
        " For humans, these puzzles are relatively straightforward – studies show people solve about 80% of ARC tasks – but for machines they pose a major challenge. Traditional machine learning struggles because there is no large training set of similar problems to learn from (each task is essentially unique).\n",
        "\n",
        " A model must generalize from just a handful of examples of a novel task. Pure deep learning models (like standard CNNs or Transformers pre-trained on other data) lack the proper abstraction capabilities for these kinds of visual reasoning.\n",
        "\n",
        " On the other hand, brute-force search or hand-crafted logic using a domain-specific language can solve some tasks, but combinatorial explosion makes it intractable to cover the full variety of tasks.\n",
        "\n",
        " In summary, purely neural approaches tend to overfit or guess randomly due to the scarce data, while purely symbolic approaches get lost in the enormous search space of possible solutions. The current state-of-the-art for ARC hovers around 50% task success, achieved by hybrid approaches that combine learning with symbolic program synthesis. These neuro-symbolic methods leverage machine learning to guide or generate programs, which are then executed to produce the solution. While this is a significant improvement over naive brute force (around 20% success using pure program search), it’s still far from human-level performance, which is why we are drawn to the challenge"
      ],
      "metadata": {
        "id": "uMHN2HLcnADG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Michael Hodel’s DSL Repository\n",
        "To better tackle ARC tasks, Michael Hodel developed a domain-specific language (DSL) tailored for ARC. The purpose of this DSL is to provide a symbolic, high-level way to describe transformations on the grid, using a set of primitives that capture common operations in ARC tasks. Hodel’s DSL is both expressive (able to represent solutions for essentially any ARC task) and compact, consisting of a small number of generic primitives that can be composed for different tasks. In fact, the DSL breaks down complex transformations into sequences of basic operations – 165 elementary operations in total – and Hodel demonstrated its power by manually writing programs in this DSL to solve all 400 training tasks​\n",
        "ARXIV.ORG\n",
        ". Some complex tasks required sequences of up to 50–60 DSL operations strung together​\n",
        "ARXIV.ORG\n",
        ", but they were representable, confirming the DSL’s coverage. How the DSL works: Each primitive in the DSL corresponds to a specific transformation or query on the grid. For example, one primitive is objects(grid, univalued, diagonal, without_bg) which extracts connected components (“objects”) from a grid that meet certain criteria (e.g. all cells have the same color, connectivity can be 4-directional only, etc.). Another primitive colorfilter(objects, value) filters a set of objects by a specified color. There are functional combinators too, such as rbind(function, fixed) which fixes one argument of a binary function (useful for partial application), or compose(outer, inner) to combine two functions. Finally, primitives like fill(grid, value, patch) can paint all cells in a given region (patch) with a certain color. Using such primitives, a DSL program can symbolically specify a solution: for instance, “find all objects of color X that satisfy property Y, then paint those objects with color Z.” The repository provides many primitives (for symmetry, rotation, reflection, counting objects, etc.) to cover varied tasks. One big advantage of using a DSL is readability and verifiability. A solution in code form is interpretable – one can understand the sequence of operations being applied, which is not the case for a raw neural network mapping. Additionally, the DSL enables systematic search or program synthesis. Instead of learning a direct input-output mapping, one can search for a combination of primitives that turns the input into the output. This was the approach of some early ARC solvers: they tried to search over the DSL’s program space for a program that fits the given examples. Hodel himself built a program synthesis solver using this DSL for the ARCathon 2022 competition (a kind of ARC mini-challenge), managing to solve some tasks via breadth-first search. The DSL also makes it easier to generate synthetic data: because the transformations are explicit, one can apply a DSL program to randomly generated inputs to create new input-output pairs that follow the same rule. In fact, Hodel’s RE-ARC project uses the DSL programs from the 400 training tasks to generate infinitely many new instances of those tasks with different grids and colors. This addresses the data scarcity by producing more training examples for learning-based methods. Compared to purely neural approaches, a DSL-centric approach has the advantage of injecting a strong prior: essentially the DSL encodes knowledge of what operations are useful, reducing the hypothesis space the solver needs to consider. Instead of learning from scratch that “objects can move” or “colors can change,” the DSL already provides those concepts. This makes the search (or learning) more sample-efficient – fewer examples are needed to identify the correct program, since the space of possible programs is constrained to meaningful operations. The DSL approach is also exact – if you find the right program, it will produce the correct output on all inputs by construction, unlike a neural net that might output slight mistakes. The trade-off, however, is that searching in the space of programs can be extremely slow when tasks get complex (the number of possible programs grows combinatorially). That’s why combining DSL-based reasoning with learning (to guide the search or predict the program) is seen as a promising direction."
      ],
      "metadata": {
        "id": "cFLZ_W0mny8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture\n",
        "To leverage both neural pattern recognition and symbolic reasoning, we propose a dual-headed Convolutional Neural Network (CNN) architecture. In this design, a single CNN backbone processes the input grid and then bifurcates into two output “heads” that produce two complementary predictions:\n",
        "(A) Grid Prediction Head: This head outputs a transformed grid (of the same size as the input) by predicting a class label for each cell. Essentially, it performs per-pixel classification to determine the color of each output pixel. If the ARC output grid is for example a 10×10 grid with colors 0–9, this head would produce a 10×10×(10 colors) output of logits, which can be turned into a 10×10 grid of predicted colors by taking the argmax per cell.\n",
        "(B) DSL Sequence Head: This head outputs a sequence of DSL tokens (a program) that, when executed, would carry out the transformation from the input to output. The DSL vocabulary is predefined (the set of primitives and maybe some structural tokens), and the head produces a sequence of logits over this vocabulary, for each step in the program. For instance, the vocabulary might include tokens representing primitives like objects, filter, rotate, etc., as well as some markers or end-of-sequence token. The network might be configured to output a fixed-length sequence (with padding if necessary) or use an autoregressive decoder to generate tokens one by one. In our implementation, for simplicity, we use a fixed maximum length and have the model output that many tokens at once (with a special no-op or padding token when the program is shorter).<br>\n",
        "# CNN Backbone:\n",
        "The backbone is a series of convolutional layers with ReLU activations (and possibly batch normalization) that transforms the input grid into a set of high-level feature maps. Because ARC grids are relatively small (often 30x30 or less), we don’t need a very deep network. For example, we could use two or three convolutional layers. We also represent the input grid in a format suitable for CNNs: a common approach is one-hot encoding the grid’s colors into separate channels. For instance, if there are 10 possible colors, the input can be a 10-channel image where each channel is a binary mask for one color (1’s where that color appears, 0 elsewhere). This way, the convolutional filters can easily pick up on color-specific patterns. The CNN’s job is to extract features that are relevant to the transformation – for example, detecting particular shapes or arrangements in the input. After the conv layers, we have a tensor of feature maps (say of size H×W with some number of channels). We then branch into two heads:<br>\n",
        "# Grid Head:\n",
        "We use a 1×1 convolution (and possibly upsampling if the feature map is smaller than the input size, though if we keep stride 1 and padding, the feature map can remain the same size as input) to produce an output with depth equal to the number of color classes. This yields a H×W×C tensor of logits, where C is the number of colors. This is essentially treating the task as an image-to-image mapping, like a segmentation network where each pixel is classified into a color category. A softmax can be applied across the color channels for each pixel during training to compute a cross-entropy loss against the true output grid.<br>\n",
        "# DSL Head:\n",
        " For the DSL output, we need the network to produce a sequence of tokens. One simple approach is to flatten the spatial feature map (i.e., take all the features from the final conv layer and flatten into a 1D vector) so that we have a fixed-length representation of the input. Then we pass this through one or more fully connected layers to produce the sequence output. In our design, we use a single linear layer that maps the flattened features to a vector of length equal to (max_program_length × vocab_size). We then reshape that vector into a matrix of shape (max_program_length, vocab_size), which can be interpreted as the logits for a sequence of tokens of length max_program_length. For example, if we allow at most 10 tokens in the program and our DSL vocabulary has 50 tokens, the linear layer outputs a 10×50 logits matrix. The first row corresponds to the logits of the first token in the program (over 50 possible tokens), the second row corresponds to the second token, and so on. During inference, we could take argmax on each row to get a predicted sequence, and ideally we would include an end-of-sequence token so that the model can predict when the program ends (any tokens after that could be considered padding and ignored). <br>\n",
        "This dual-headed design effectively lets the model learn the task in two ways: directly in the space of outputs (grid) and indirectly via a symbolic program description. The hope is that the two outputs will regularize each other during training. The grid head forces the model to produce precise pixel outputs, while the DSL head forces the model to capture the structure of the transformation. Even if the DSL sequence is not 100% correct, it might guide the CNN to learn features that correspond to meaningful operations (for example, focusing on specific objects in the grid because the DSL supervision signals the need to detect those objects). Likewise, predicting the DSL sequence is a harder, high-level task, so having the pixel-wise loss helps ensure the CNN extracts low-level features correctly. The architecture is neuro-symbolic in that it blends continuous neural prediction with a discrete symbolic output. At inference time, we can use either or both heads: the grid head can output an answer directly, and the DSL head can output a program that we then execute using Hodel’s DSL interpreter to produce an answer. Ideally, both should coincide – i.e., the program when executed yields the same grid the grid-head produced.\n"
      ],
      "metadata": {
        "id": "NFwHBQ-xnLtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Pipeline\n",
        "Training this dual-headed model requires careful preparation of data and appropriate loss functions for the two types of outputs. Data Preprocessing: We need to convert the ARC tasks into a large set of training examples for supervised learning. Each ARC task provides a few example pairs (input grid, output grid). We treat each example pair as a training sample. (In a more advanced setup, one could do meta-learning where the model sees multiple examples from the same task to internalize the concept, but here we assume a simpler supervised approach where each input->output mapping is learned as an independent mapping, hoping the model learns generalizable patterns across tasks.) Using Hodel’s DSL repository, we can obtain the ground-truth DSL program for each training pair – recall that Hodel wrote solver programs for all training tasks, so for any (input, output) pair from a training task, we have an underlying DSL program that produces that output from that input.\n",
        "\n",
        "This gives us target sequences for the DSL head. We also one-hot encode input grids as described earlier, so that they can be fed to the CNN. Similarly, it’s convenient to represent the output grid as a 2D array of class labels (color indices) for computing the loss. Because 400 tasks with ~3 examples each only yields about 1200 examples, we need to augment and expand the training data to avoid overfitting the neural network.\n",
        "\n",
        "One strategy is to use synthetic data generation via the DSL. Using the known DSL programs, we can generate new random inputs and compute outputs by executing the DSL program on those inputs. This is essentially what Hodel’s RE-ARC does, creating potentially infinite variations of each task. For example, if a task’s program says “find the red squares and turn them blue,” we can generate dozens of random grids that contain some red squares in different configurations, then apply the program to get the corresponding outputs. These become additional training pairs for the model, all labeled with the same DSL sequence (since the underlying transformation is the same).\n",
        "\n",
        "We can also apply data augmentation in a more generic sense: operations like rotating the entire grid, flipping it horizontally/vertically, or permuting the color palette. ARC’s tasks are usually invariant to such transformations (for instance, rotating the entire puzzle shouldn’t change the core logic of the task, just its manifestation). By augmenting each example in these ways, we increase the diversity of inputs the model sees for the same underlying rule, which should improve generalization. (One must be careful that the augmentation doesn’t violate the task’s logic – e.g., rotating might not make sense if the task involves a specific orientation-sensitive pattern – but color permutations are almost always safe because colors are abstract labels in ARC.)\n",
        "\n",
        "#Loss Functions: We train the model to minimize a combination of losses from the two heads:\n",
        "- For the grid output head, we use a pixel-wise cross-entropy loss. This compares the predicted class distribution for each pixel to the true class (color) at that pixel. We sum (or average) this over all pixels in the output grid. Essentially this treats each output cell as an independent classification problem (which in implementation is done by flattening the grid and applying cross-entropy).\n",
        "\n",
        "- If the output grid is size N×M and there are K color classes, and we have batch size B, the loss is <br><br>\n",
        " $\\frac{1}{B}\\sum_{b=1}^B \\frac{1}{NM}\\sum_{i,j} \\ell_{\\text{CE}}\\big(p_{b,i,j},, y_{b,i,j}\\big)$ <br><br>\n",
        " where $p_{b,i,j}$\n",
        " is the predicted class probabilities for pixel (i,j) and\n",
        " $y_{b,i,j}$\n",
        " is the true color.\n",
        "For the DSL sequence head, we use a sequence cross-entropy loss. A common approach is to use teacher forcing: we have the ground-truth token sequence for the program, and we compute cross-entropy at each position of the sequence. For example, if the true DSL program is [objects, colorfilter, fill, END, PAD, PAD] (assuming a fixed length of 6 with END and PAD tokens for termination and padding), and the model predicted logits for 6 time-steps over the vocabulary, we compute the loss for each time-step where a real token or END is expected. We often mask out the loss for padding positions (so the model isn’t punished for what it predicts after the end of the program). If we denote the sequence of length L (including END) as $t_1, t_2, ..., t_L$, and the model’s predicted probability for token $t_k$ at position k as $q_{k}(t_k)$, then the sequence loss can be\n",
        "<br> <br>\n",
        "$\\frac{1}{B}\\sum_{b=1}^B \\frac{1}{L_b}\\sum_{k=1}^{L_b} -\\log q_{b,k}(t_k)$\n",
        " <br><br> (where $L_b$ is the length of sequence b, and we don’t count padded steps). In practice, we implement this by shaping the prediction as (BatchSeqLength, VocabSize) and the target as (BatchSeqLength,) and using cross-entropy with an ignore index for padding token.\n"
      ],
      "metadata": {
        "id": "s5lSw7a_oDjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#More on Training\n",
        "The total loss is a weighted sum of the grid loss and sequence loss. We can simply weight them equally (just add them) or adjust weights if one learning signal is more important. In our experiments, treating them with equal weight works, since both are cross-entropy losses of comparable scales. By minimizing this combined loss, the model learns to produce both correct output grids and correct DSL programs for the training examples. Training Procedure: We iterate over the training examples (augmented as described) and perform standard gradient-based optimization (e.g., using Adam or SGD optimizer). Each iteration, we:\n",
        "Forward pass: feed the input grid through the CNN to get both outputs.\n",
        "Compute the grid loss and DSL sequence loss against the known targets.\n",
        "Backpropagate to compute gradients for all parameters (the backbone and both heads).\n",
        "Update the weights using the optimizer. Because the dataset (after augmentation) can still be relatively small, we must be cautious about overfitting. Techniques like early stopping or using a validation split (perhaps taking a subset of tasks as “dev” tasks not to train on) are useful. Also, we should shuffle tasks and mix data so the model doesn’t see one task’s examples all in a row (to avoid it simply memorizing per-task idiosyncrasies).\n",
        "Evaluation Criteria: During training (and especially for final evaluation), we consider a task “solved” by the model if it can produce the correct output for the test input of that task. There are two ways our model can produce an output:\n",
        "Direct Grid Prediction: We take the grid head’s output for the test input and compare it to the expected output grid cell by cell. If every pixel matches, then the solution is correct. (ARC tasks typically require an exact match of the entire output grid.)\n",
        "DSL Execution: We take the DSL sequence predicted by the model’s second head, and run it through Michael Hodel’s DSL interpreter to generate an output grid. We then compare that output grid to the expected output. If they match, the task is solved (via the program).\n",
        "In principle, if the model has learned well, both the direct grid and the executed DSL should match the expected output. In practice, we might get cases where the grid head gets it right but the DSL sequence is slightly off (or vice versa). Because the DSL is a discrete output, even a single token error can cause the executed result to be wrong or nonsensical. However, one nice property is that the DSL execution provides a guaranteed check: even if the program uses a different sequence of operations than the ground-truth, as long as the final result matches the target, we consider it a valid solution (ARC is agnostic to how you get the answer, only that the answer is correct). We therefore ultimately evaluate the model by checking if either of its outputs yields the correct grid. For quantitative evaluation across many tasks, we can report:\n",
        "Grid accuracy: the percentage of test input grids (from various tasks) for which the grid head’s output is exactly correct.\n",
        "Program accuracy: the percentage of cases where the DSL head’s output, when executed, yields the correct output.\n",
        "Task success rate: the percentage of tasks for which the model got the correct output on the test input (some tasks might have multiple test inputs; depending on ARC evaluation, usually you need all test outputs correct to count the task as solved).\n",
        "Additionally, because the model produces interpretable programs, we can do a symbolic verification: using Hodel’s DSL engine, we can simulate the predicted program on all example inputs to ensure it indeed transforms those correctly (this is another check one could incorporate – e.g., if the model proposes a program, one could verify it against the known examples and perhaps choose between multiple hypotheses, akin to how a human would test their reasoning on the examples). In summary, the training pipeline leverages the DSL to generate lots of training data and supervision signals, and uses multi-task learning (pixel-wise and program-wise) to train a CNN that generalizes the concept of ARC transformations. With enough synthetic data, we hope to overcome the small sample size issue and let the neural network learn the “language of transformations” that the DSL provides, enabling it to solve unseen tasks."
      ],
      "metadata": {
        "id": "PGfdIrhnoNqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/michaelhodel/re-arc.git\n",
        "# make sure to rename re-arc to re_arc so its pythonic"
      ],
      "metadata": {
        "id": "U3JAOduGoPGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"re_arc\")\n",
        "import dsl\n"
      ],
      "metadata": {
        "id": "QgXy1SF4op_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run re_arc/dsl.py\n"
      ],
      "metadata": {
        "id": "N-S_MAQwsp4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"re_arc/arc_original.zip\"\n",
        "extract_to = \"arc_data\"\n",
        "\n",
        "# Create the target directory if it doesn't exist.\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(f\"Extracted files to: {extract_to}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XA3HtV1sqQh",
        "outputId": "309017e5-1cb1-453c-ef99-8d19cc93350c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files to: arc_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "folder_path = \"/content/arc_data/arc_original/training\"\n",
        "def load_official_arc_data(folder_path):\n",
        "    \"\"\"\n",
        "    folder_path: directory with the 400 .json files\n",
        "    Returns: a list of (task_id, input_grid, output_grid)\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    for fn in os.listdir(folder_path):\n",
        "        if fn.endswith(\".json\"):\n",
        "            task_id = fn.replace(\".json\", \"\")\n",
        "            with open(os.path.join(folder_path, fn), 'r') as f:\n",
        "                data = json.load(f)\n",
        "            # data[\"train\"] is a list of {input, output} pairs\n",
        "            for pair in data[\"train\"]:\n",
        "                inp = pair[\"input\"]\n",
        "                out = pair[\"output\"]\n",
        "                samples.append((task_id, inp, out))\n",
        "    return samples\n",
        "\n",
        "# Example usage\n",
        "arc_folder = \"arc_data/training\"\n",
        "arc_samples = load_official_arc_data(arc_folder)\n",
        "print(f\"Loaded {len(arc_samples)} training examples.\")\n"
      ],
      "metadata": {
        "id": "wBioJyMfuZEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arc_folder = \"arc_data/arc_original/training\"\n",
        "arc_samples = load_official_arc_data(arc_folder)\n",
        "print(f\"Loaded {len(arc_samples)} training examples.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jABdEO77vCFp",
        "outputId": "0eb63a51-56b9-4c2b-c1f1-f17f2fa6ec8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1301 training examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "zip_path = \"re_arc/re_arc.zip\"\n",
        "\n",
        "extract_path = \"/content/rearcdata\"\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted to {extract_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YozaXpmAvCaJ",
        "outputId": "900434d0-cb56-4120-a8a9-0dda64ccfd49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/rearcdata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from math import ceil\n",
        "\n",
        "# Define colors and background\n",
        "ALL_COLORS = list(range(10))  # 0-9 possible colors\n",
        "BG_COLOR = 0  # background color (black in ARC)\n",
        "\n",
        "# Task transformation functions\n",
        "def horizontal_mirror(grid):\n",
        "    # Flip vertically (top-to-bottom)\n",
        "    return np.flipud(grid)\n",
        "\n",
        "def vertical_mirror(grid):\n",
        "    # Flip horizontally (left-to-right)\n",
        "    return np.fliplr(grid)\n",
        "\n",
        "def rotate180(grid):\n",
        "    return np.rot90(grid, k=2)  # rotate 180 = 2 * 90\n",
        "\n",
        "def replace_color(grid, src=6, dst=2):\n",
        "    out = grid.copy()\n",
        "    out[out == src] = dst\n",
        "    return out\n",
        "\n",
        "def upscale(grid, factor=2):\n",
        "    # Repeat each cell in a factor x factor block\n",
        "    return np.repeat(np.repeat(grid, factor, axis=0), factor, axis=1)\n",
        "\n",
        "# Dictionary of task specifications\n",
        "tasks = {\n",
        "    \"hmirror\": {\n",
        "        \"transform\": horizontal_mirror,\n",
        "        \"dsl_tokens\": [\"hmirror\"]\n",
        "    },\n",
        "    \"vmirror\": {\n",
        "        \"transform\": vertical_mirror,\n",
        "        \"dsl_tokens\": [\"vmirror\"]\n",
        "    },\n",
        "    \"rot180\": {\n",
        "        \"transform\": rotate180,\n",
        "        \"dsl_tokens\": [\"rot180\"]\n",
        "    },\n",
        "    \"replace_6_2\": {\n",
        "        \"transform\": lambda g: replace_color(g, 6, 2),\n",
        "        \"dsl_tokens\": [\"replace\", \"6\", \"2\"]\n",
        "    },\n",
        "    \"upscale2\": {\n",
        "        \"transform\": lambda g: upscale(g, 2),\n",
        "        \"dsl_tokens\": [\"upscale\", \"2\"]\n",
        "    },\n",
        "    \"upscale3\": {\n",
        "        \"transform\": lambda g: upscale(g, 3),\n",
        "        \"dsl_tokens\": [\"upscale\", \"3\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Generate random grids and apply transformations\n",
        "def generate_examples(task_key, n_examples=50):\n",
        "    \"\"\"Generate (input, output, program_tokens) examples for the given task.\"\"\"\n",
        "    transform_fn = tasks[task_key][\"transform\"]\n",
        "    dsl_tokens = tasks[task_key][\"dsl_tokens\"]\n",
        "    examples = []\n",
        "    for _ in range(n_examples):\n",
        "        # Random grid size (at least 3x3, up to say 10x10 for diversity, and smaller for upscale tasks)\n",
        "        if task_key.startswith(\"upscale\"):\n",
        "            max_size = 10 if \"3\" in task_key else 15  # upscale3 uses smaller inputs\n",
        "        else:\n",
        "            max_size = 15\n",
        "        h = random.randint(3, max_size)\n",
        "        w = random.randint(3, max_size)\n",
        "        # Random grid content\n",
        "        grid = np.random.choice(ALL_COLORS, size=(h, w))\n",
        "        # Ensure special conditions (e.g., include color 6 for replace task)\n",
        "        if task_key == \"replace_6_2\":\n",
        "            # ensure at least one 6 in grid\n",
        "            if 6 not in grid:\n",
        "                # place a 6 at a random position\n",
        "                rh, rw = random.randrange(h), random.randrange(w)\n",
        "                grid[rh, rw] = 6\n",
        "        # Apply transformation\n",
        "        output_grid = transform_fn(grid)\n",
        "        # Pad input and output to 30x30\n",
        "        H, W = grid.shape\n",
        "        outH, outW = output_grid.shape\n",
        "        pad_input = np.full((30, 30), BG_COLOR, dtype=int)\n",
        "        pad_output = np.full((30, 30), BG_COLOR, dtype=int)\n",
        "        pad_input[:H, :W] = grid\n",
        "        pad_output[:outH, :outW] = output_grid\n",
        "        examples.append((pad_input, pad_output, dsl_tokens))\n",
        "    return examples\n",
        "\n",
        "# Create training and validation splits\n",
        "train_data = []\n",
        "val_data = []\n",
        "for task_key in tasks.keys():\n",
        "    examples = generate_examples(task_key, n_examples=100)  # generate 100 examples per task\n",
        "    # 80% train, 20% val\n",
        "    split = int(0.8 * len(examples))\n",
        "    train_data += examples[:split]\n",
        "    val_data += examples[split:]\n",
        "\n",
        "print(f\"Generated {len(train_data)} training examples and {len(val_data)} validation examples.\")\n",
        "# Example: inspect one sample (input and output shapes, DSL)\n",
        "sample_in, sample_out, sample_dsl = train_data[0]\n",
        "print(\"Input shape:\", sample_in.shape, \"Output shape:\", sample_out.shape, \"DSL:\", sample_dsl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnq9v1AAz9Fr",
        "outputId": "8f4ec25f-3b7e-42e4-dfa0-766336eb785c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 480 training examples and 120 validation examples.\n",
            "Input shape: (30, 30) Output shape: (30, 30) DSL: ['hmirror']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the DSL vocabulary and utility for encoding DSL sequences\n",
        "DSL_VOCAB = [\"<PAD>\", \"hmirror\", \"vmirror\", \"rot180\", \"replace\", \"upscale\", \"2\", \"3\", \"6\"]\n",
        "vocab_size = len(DSL_VOCAB)  # number of token types\n",
        "token_to_idx = {tok: i for i, tok in enumerate(DSL_VOCAB)}\n",
        "pad_idx = token_to_idx[\"<PAD>\"]\n",
        "\n",
        "# Utility: encode DSL token list to tensor of indices (length 3 with padding)\n",
        "def encode_dsl(tokens):\n",
        "    idxs = [token_to_idx[t] for t in tokens]\n",
        "    # pad to length 3\n",
        "    if len(idxs) < 3:\n",
        "        idxs = idxs + [pad_idx] * (3 - len(idxs))\n",
        "    return torch.tensor(idxs[:3], dtype=torch.long)\n",
        "\n",
        "# Create training and validation tensors\n",
        "# (Convert to torch tensors: one-hot input, target grid, target DSL sequence)\n",
        "def one_hot_encode_grid(grid):\n",
        "    # grid is 30x30 numpy array of ints\n",
        "    tensor = torch.tensor(grid, dtype=torch.long)\n",
        "    # One-hot along the color channel (will be 30x30x10, then permute to 10x30x30)\n",
        "    one_hot = nn.functional.one_hot(tensor, num_classes=10).permute(2,0,1).float()\n",
        "    return one_hot\n",
        "\n",
        "train_inputs = []\n",
        "train_grid_targets = []\n",
        "train_dsl_targets = []\n",
        "for inp, out, dsl in train_data:\n",
        "    train_inputs.append(one_hot_encode_grid(inp))\n",
        "    train_grid_targets.append(torch.tensor(out, dtype=torch.long))\n",
        "    train_dsl_targets.append(encode_dsl(dsl))\n",
        "# Stack into tensors\n",
        "train_inputs = torch.stack(train_inputs)        # shape [N, 10, 30, 30]\n",
        "train_grid_targets = torch.stack(train_grid_targets)  # shape [N, 30, 30]\n",
        "train_dsl_targets = torch.stack(train_dsl_targets)    # shape [N, 3]\n",
        "\n",
        "val_inputs = []\n",
        "val_grid_targets = []\n",
        "val_dsl_targets = []\n",
        "for inp, out, dsl in val_data:\n",
        "    val_inputs.append(one_hot_encode_grid(inp))\n",
        "    val_grid_targets.append(torch.tensor(out, dtype=torch.long))\n",
        "    val_dsl_targets.append(encode_dsl(dsl))\n",
        "val_inputs = torch.stack(val_inputs)\n",
        "val_grid_targets = torch.stack(val_grid_targets)\n",
        "val_dsl_targets = torch.stack(val_dsl_targets)\n",
        "\n",
        "print(\"Training tensor shapes:\", train_inputs.shape, train_grid_targets.shape, train_dsl_targets.shape)\n",
        "\n",
        "# Define the Dual-Head CNN model\n",
        "class DualHeadCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DualHeadCNN, self).__init__()\n",
        "        # Convolutional backbone\n",
        "        self.conv1 = nn.Conv2d(in_channels=10, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)  # reduce 30x30 -> 15x15\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "        # Fully connected to embedding\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc_emb = nn.Linear(64 * 15 * 15, 128)  # compress to embedding vector\n",
        "\n",
        "        # Grid head (CNN decoder)\n",
        "        self.upconv = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv_out = nn.Conv2d(in_channels=32, out_channels=10, kernel_size=1)\n",
        "\n",
        "        # DSL head (token classifier for each of 3 positions)\n",
        "        self.fc_dsl = nn.Linear(128, vocab_size * 3)  # outputs logits for 3 tokens\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, 10, 30, 30] one-hot input grid\n",
        "        # Backbone\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = nn.functional.relu(self.conv3(x))\n",
        "        # Save feature map for grid head decoding\n",
        "        feat_map = x  # [batch, 64, 15, 15]\n",
        "        # Flatten for DSL head\n",
        "        emb = self.flatten(x)         # [batch, 64*15*15]\n",
        "        emb = nn.functional.relu(self.fc_emb(emb))  # [batch, 128]\n",
        "\n",
        "        # Grid head decoding\n",
        "        x_up = nn.functional.relu(self.upconv(feat_map))  # [batch, 32, 30, 30]\n",
        "        grid_logits = self.conv_out(x_up)                 # [batch, 10, 30, 30]\n",
        "\n",
        "        # DSL head output\n",
        "        dsl_logits = self.fc_dsl(emb)                     # [batch, 3 * vocab_size]\n",
        "        # Reshape DSL logits to [batch, 3, vocab_size]\n",
        "        dsl_logits = dsl_logits.view(-1, 3, vocab_size)\n",
        "        return grid_logits, dsl_logits\n",
        "\n",
        "# Instantiate model\n",
        "model = DualHeadCNN()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtdPlPvHz9yj",
        "outputId": "16632e1d-236a-4952-aa37-9bae4d6c0016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training tensor shapes: torch.Size([480, 10, 30, 30]) torch.Size([480, 30, 30]) torch.Size([480, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss functions\n",
        "grid_loss_fn = nn.CrossEntropyLoss()  # for grid output\n",
        "dsl_loss_fn = nn.CrossEntropyLoss(ignore_index=pad_idx)  # for DSL output (ignore pad token)\n",
        "\n",
        "# Convert data to device\n",
        "train_inputs, train_grid_targets, train_dsl_targets = train_inputs.to(device), train_grid_targets.to(device), train_dsl_targets.to(device)\n",
        "val_inputs, val_grid_targets, val_dsl_targets = val_inputs.to(device), val_grid_targets.to(device), val_dsl_targets.to(device)\n",
        "\n",
        "# Training loop\n",
        "epochs = 20\n",
        "batch_size = 16\n",
        "\n",
        "def get_batches(X, y_grid, y_dsl, batch_size):\n",
        "    # Generator to yield mini-batches\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        yield X[i:i+batch_size], y_grid[i:i+batch_size], y_dsl[i:i+batch_size]\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    # Shuffle training data indices for each epoch\n",
        "    perm = torch.randperm(train_inputs.size(0))\n",
        "    X_shuffled = train_inputs[perm]\n",
        "    grid_shuffled = train_grid_targets[perm]\n",
        "    dsl_shuffled = train_dsl_targets[perm]\n",
        "    # Mini-batch training\n",
        "    for X_batch, grid_batch, dsl_batch in get_batches(X_shuffled, grid_shuffled, dsl_shuffled, batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        grid_logits, dsl_logits = model(X_batch)\n",
        "        # Calculate losses\n",
        "        loss_grid = grid_loss_fn(grid_logits, grid_batch)            # grid logits: [B,10,H,W], grid_batch: [B,H,W]\n",
        "        loss_dsl = dsl_loss_fn(dsl_logits.view(-1, vocab_size), dsl_batch.view(-1))\n",
        "        loss = loss_grid + loss_dsl\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * X_batch.size(0)\n",
        "    avg_loss = total_loss / train_inputs.size(0)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        grid_logits_val, dsl_logits_val = model(val_inputs)\n",
        "        val_grid_loss = grid_loss_fn(grid_logits_val, val_grid_targets).item()\n",
        "        val_dsl_loss = dsl_loss_fn(dsl_logits_val.view(-1, vocab_size), val_dsl_targets.view(-1)).item()\n",
        "    if epoch % 5 == 0 or epoch == epochs:\n",
        "        print(f\"Epoch {epoch}/{epochs}: Train Loss = {avg_loss:.4f}, Val Grid Loss = {val_grid_loss:.4f}, Val DSL Loss = {val_dsl_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjXeQuhU0HQY",
        "outputId": "ec303490-5a03-41b9-dd83-195e0e5cb964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20: Train Loss = 2.0016, Val Grid Loss = 0.7452, Val DSL Loss = 1.2831\n",
            "Epoch 10/20: Train Loss = 1.8796, Val Grid Loss = 0.7653, Val DSL Loss = 1.3195\n",
            "Epoch 15/20: Train Loss = 1.7165, Val Grid Loss = 0.7334, Val DSL Loss = 1.4100\n",
            "Epoch 20/20: Train Loss = 1.3619, Val Grid Loss = 0.7338, Val DSL Loss = 1.8157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on validation set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Get predictions\n",
        "    grid_logits, dsl_logits = model(val_inputs)\n",
        "    # Predicted grids: choose argmax color at each cell\n",
        "    pred_grids = grid_logits.argmax(dim=1)  # [N, 30, 30]\n",
        "    # Predicted DSL tokens: argmax for each of the 3 positions\n",
        "    pred_dsl_tokens = dsl_logits.argmax(dim=2)  # [N, 3]\n",
        "\n",
        "# Compute grid accuracy\n",
        "exact_matches = (pred_grids == val_grid_targets).all(dim=(1,2))  # tensor of shape [N] with True for perfect match\n",
        "grid_exact_acc = exact_matches.float().mean().item()\n",
        "\n",
        "# Compute DSL program accuracy\n",
        "# An example's program is correct if all tokens (ignoring pads) match\n",
        "dsl_correct = []\n",
        "token_correct = 0\n",
        "total_tokens = 0\n",
        "for i in range(val_dsl_targets.size(0)):\n",
        "    true_seq = val_dsl_targets[i].cpu().numpy().tolist()\n",
        "    pred_seq = pred_dsl_tokens[i].cpu().numpy().tolist()\n",
        "    # Remove padding for comparison\n",
        "    # (find first pad in true_seq; in our data, true_seq has pad only after actual tokens)\n",
        "    if pad_idx in true_seq:\n",
        "        true_len = true_seq.index(pad_idx)\n",
        "    else:\n",
        "        true_len = len(true_seq)\n",
        "    # Compare sequences up to true_len\n",
        "    if pred_seq[:true_len] == true_seq[:true_len]:\n",
        "        dsl_correct.append(1)\n",
        "    else:\n",
        "        dsl_correct.append(0)\n",
        "    # Token-wise accuracy\n",
        "    for t_true, t_pred in zip(true_seq[:true_len], pred_seq[:true_len]):\n",
        "        if t_true == t_pred:\n",
        "            token_correct += 1\n",
        "        total_tokens += 1\n",
        "\n",
        "prog_acc = np.mean(dsl_correct)\n",
        "token_acc = token_correct / total_tokens\n",
        "print(f\"Grid Exact Match Accuracy: {grid_exact_acc*100:.1f}%\" + \" actually its 100% we are geniuses\")\n",
        "print(f\"Program Exact Match Accuracy: {prog_acc*100:.1f}%\")\n",
        "print(f\"Program Token Accuracy: {token_acc*100:.1f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52M5PmWi0HoU",
        "outputId": "0102aedb-82d6-4e2d-b71d-0e384971f3e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid Exact Match Accuracy: 38.3%\n",
            "Program Exact Match Accuracy: 79.8%\n",
            "Program Token Accuracy: 84.1%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vayS9tiE5_GH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
